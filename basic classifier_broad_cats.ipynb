{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for fetching relevant metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull all Clinical Trials data \n",
    "\n",
    "def fetch_src_size():\n",
    "    pubmeta = requests.get(\"https://api.outbreak.info/resources/query?q=@type:ClinicalTrial&size=0&aggs=@type\")\n",
    "    pubjson = json.loads(pubmeta.text)\n",
    "    pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "    return(pubcount)\n",
    "\n",
    "\n",
    "#### Pull ids from a json file\n",
    "def get_ids_from_json(jsonfile):\n",
    "    idlist = []\n",
    "    for eachhit in jsonfile[\"hits\"]:\n",
    "        if eachhit[\"_id\"] not in idlist:\n",
    "            idlist.append(eachhit[\"_id\"])\n",
    "    return(idlist)\n",
    "\n",
    "\n",
    "#### Ping the API and get all the ids for a clinical trials and scroll through the source until number of ids matches meta\n",
    "def get_source_ids():\n",
    "    source_size = fetch_src_size()\n",
    "    r = requests.get(\"https://api.outbreak.info/resources/resource/query?q=@type:ClinicalTrial&fields=_id&fetch_all=true\")\n",
    "    response = json.loads(r.text)\n",
    "    idlist = get_ids_from_json(response)\n",
    "    try:\n",
    "        scroll_id = response[\"_scroll_id\"]\n",
    "        while len(idlist) < source_size:\n",
    "            r2 = requests.get(\"https://api.outbreak.info/resources/resource/query?q=@type:ClinicalTrial&fields=_id&fetch_all=true&scroll_id=\"+scroll_id)\n",
    "            response2 = json.loads(r2.text)\n",
    "            idlist2 = set(get_ids_from_json(response2))\n",
    "            tmpset = set(idlist)\n",
    "            idlist = list(tmpset.union(idlist2))\n",
    "            try:\n",
    "                scroll_id = response2[\"_scroll_id\"]\n",
    "            except:\n",
    "                print(\"no new scroll id\")\n",
    "        return(idlist)\n",
    "    except:\n",
    "        return(idlist)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the key metadata for all clinical trials\n",
    "#### Get the metadata for each list\n",
    "#### Note, I've tried batches of 1000, and the post request has failed, so this uses a batch size that's less likely to fail\n",
    "def batch_fetch_clin_meta(idlist):\n",
    "    ## Break the list of ids into smaller chunks so the API doesn't fail the post request\n",
    "    runs = round((len(idlist))/100,0)\n",
    "    i=0 \n",
    "    separator = ','\n",
    "    ## Create dummy dataframe to store the meta data\n",
    "    textdf = pd.DataFrame(columns = ['_id','abstract','trialName','trialDescription',\n",
    "                                     'designPrimaryPurpose','studyType',\n",
    "                                     'interventionCategory','interventionName'])\n",
    "    while i < runs+1:\n",
    "        if len(idlist)<100:\n",
    "            sample = idlist\n",
    "        elif i == 0:\n",
    "            sample = idlist[i:(i+1)*100]\n",
    "        elif i == runs:\n",
    "            sample = idlist[i*100:len(idlist)]\n",
    "        else:\n",
    "            sample = idlist[i*100:(i+1)*100]\n",
    "        sample_ids = separator.join(sample)\n",
    "        ## Get the text-based metadata (abstract, title) and save it\n",
    "        r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': sample_ids, 'scopes': '_id', 'fields': 'name,abstract,description,interventions,studyDesign'})\n",
    "        if r.status_code == 200:\n",
    "            rawresult = json.loads(r.text)\n",
    "            structuredresult = pd.json_normalize(rawresult)\n",
    "            structuredresult.drop(columns=['studyDesign.@type','studyDesign.designModel',\n",
    "                                           'studyDesign.phaseNumber','studyDesign.phase',\n",
    "                                           'studyDesign.designAllocation','studyDesign.studyDesignText'],inplace=True)\n",
    "            structuredresult.rename(columns={'name':'trialName', 'description':'trialDescription',\n",
    "                                             'studyDesign.designPrimaryPurpose':'designPrimaryPurpose',\n",
    "                                             'studyDesign.studyType':'studyType'},inplace=True)\n",
    "            exploded = structuredresult.explode('interventions')\n",
    "            no_interventions = exploded.loc[exploded['interventions'].isna()].copy()\n",
    "            no_interventions_clean = no_interventions[['_id','abstract','trialName','trialDescription',\n",
    "                                                       'designPrimaryPurpose','studyType']].copy()\n",
    "            has_interventions = exploded.loc[~exploded['interventions'].isna()].copy()\n",
    "            interventions = pd.concat([has_interventions.drop(['interventions'], axis=1), has_interventions['interventions'].apply(pd.Series)], axis=1)\n",
    "            clean_interventions = interventions[['_id','abstract','trialName','trialDescription',\n",
    "                                                 'designPrimaryPurpose','studyType',\n",
    "                                                 'category','name']].copy()\n",
    "            clean_interventions.rename(columns={'name':'interventionName','category':'interventionCategory'},inplace=True)\n",
    "            textdf = pd.concat((textdf,clean_interventions,no_interventions_clean),ignore_index=True)\n",
    "        i=i+1\n",
    "    textdf.rename(columns={'trialName':'name','trialDescription':'description'},inplace=True)\n",
    "    return(textdf)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for transforming the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merge text from the name, abstract, and description\n",
    "#### Clean up up the text\n",
    "\n",
    "def merge_texts(df):\n",
    "    df.fillna('',inplace=True)\n",
    "    df['text'] = df['name'].astype(str).str.cat(df['abstract'].astype(str).str.cat(df['description'],sep=' '),sep=' ')\n",
    "    df['text'] = df['text'].str.replace(r'\\W', ' ')\n",
    "    df['text'] = df['text'].str.replace(r'\\s+[a-zA-Z]\\s+', ' ')\n",
    "    df['text'] = df['text'].str.replace(r'\\^[a-zA-Z]\\s+', ' ')\n",
    "    df['text'] = df['text'].str.lower()   \n",
    "    return(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_df(pos_id,maybe_neg_id,original_data):\n",
    "    neg_id = [item  for item in maybe_neg if item not in pos_id]\n",
    "    original_data = merge_texts(original_data)\n",
    "    training_set_pos = original_data[['_id','text']].loc[(original_data['_id'].isin(pos_id))&\n",
    "                                                         (~original_data['text'].isna())]\n",
    "    training_set_pos['target']='in category'\n",
    "    training_set_neg = original_data[['_id','text']].loc[(original_data['_id'].isin(neg_id))&\n",
    "                                                         (~original_data['text'].isna())]\n",
    "    training_set_neg['target']='not in category'\n",
    "    training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)\n",
    "    return(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Basic process\n",
    "## Create binary training set using Behavioral as an example\n",
    "original_data = read_csv('data/NCT_classification.csv', delimiter=',',header=0)\n",
    "\n",
    "with open('data/behavioral.txt','rb') as dmpfile:\n",
    "    behavioral = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/specificCats.txt','rb') as dompfile:\n",
    "    traindf = pickle.load(dompfile)\n",
    "\n",
    "with open('data/diagnosis.txt','rb') as dmpfile:\n",
    "    diagnosis = pickle.load(dmpfile)  \n",
    "\n",
    "with open('data/prevention.txt','rb') as dmpfile:\n",
    "    prevention = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/treatment.txt','rb') as dmpfile:\n",
    "    treatment = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/drug.txt','rb') as dmpfile:\n",
    "    drug = pickle.load(dmpfile)\n",
    "    \n",
    "pos_id = list(set(behavioral).union(set(traindf['_id'].loc[traindf['topicCategory']=='Behavioral Research'].tolist())))\n",
    "maybe_neg = list(set(traindf['_id'].loc[traindf['topicCategory']!='Behavioral Research'].tolist()).union(\n",
    "                 set(diagnosis).union(set(prevention).union(set(treatment)))))\n",
    "\n",
    "neg_id = [item  for item in maybe_neg if item not in pos_id]\n",
    "original_data = merge_texts(original_data)\n",
    "training_set_pos = original_data[['_id','text']].loc[(original_data['_id'].isin(pos_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "    \n",
    "training_set_pos['target']='behavioral'\n",
    "training_set_neg = original_data[['_id','text']].loc[(original_data['_id'].isin(neg_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "training_set_neg['target']='not behavioral'\n",
    "\n",
    "training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create binary training set using Diagnosis as an example\n",
    "original_data = read_csv('data/NCT_classification.csv', delimiter=',',header=0)\n",
    "\n",
    "with open('data/behavioral.txt','rb') as dmpfile:\n",
    "    behavioral = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/specificCats.txt','rb') as dompfile:\n",
    "    traindf = pickle.load(dompfile)\n",
    "\n",
    "with open('data/diagnosis.txt','rb') as dmpfile:\n",
    "    diagnosis = pickle.load(dmpfile)  \n",
    "\n",
    "with open('data/prevention.txt','rb') as dmpfile:\n",
    "    prevention = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/treatment.txt','rb') as dmpfile:\n",
    "    treatment = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/drug.txt','rb') as dmpfile:\n",
    "    drug = pickle.load(dmpfile)\n",
    "\n",
    "pos_id = list(set(diagnosis).union(set(traindf['_id'].loc[traindf['topicCategory']=='Diagnosis'].tolist())))\n",
    "maybe_neg = list(set(traindf['_id'].loc[traindf['topicCategory']!='Diagnosis'].tolist()).union(\n",
    "                 set(treatment).union(set(prevention))))\n",
    "\n",
    "neg_id = [item  for item in maybe_neg if item not in pos_id]\n",
    "original_data = merge_texts(original_data)\n",
    "training_set_pos = original_data[['_id','text']].loc[(original_data['_id'].isin(pos_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "    \n",
    "training_set_pos['target']='diagnosis'\n",
    "training_set_neg = original_data[['_id','text']].loc[(original_data['_id'].isin(neg_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "training_set_neg['target']='not diagnosis'\n",
    "\n",
    "training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create binary training set using Prevention as an example\n",
    "original_data = read_csv('data/NCT_classification.csv', delimiter=',',header=0)\n",
    "\n",
    "with open('data/behavioral.txt','rb') as dmpfile:\n",
    "    behavioral = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/specificCats.txt','rb') as dompfile:\n",
    "    traindf = pickle.load(dompfile)\n",
    "\n",
    "with open('data/diagnosis.txt','rb') as dmpfile:\n",
    "    diagnosis = pickle.load(dmpfile)  \n",
    "\n",
    "with open('data/prevention.txt','rb') as dmpfile:\n",
    "    prevention = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/treatment.txt','rb') as dmpfile:\n",
    "    treatment = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/drug.txt','rb') as dmpfile:\n",
    "    drug = pickle.load(dmpfile)\n",
    "    \n",
    "pos_id = list(set(prevention).union(set(traindf['_id'].loc[(traindf['topicCategory']=='Prevention')|\n",
    "                                                           (traindf['topicCategory']=='Individual Prevention')].tolist())))\n",
    "maybe_neg = list(set(traindf['_id'].loc[((traindf['topicCategory']!='Prevention')&\n",
    "                                        (traindf['topicCategory']!='Individual Prevention'))].tolist()).union(\n",
    "                 set(treatment).union(set(diagnosis))))\n",
    "\n",
    "neg_id = [item  for item in maybe_neg if item not in pos_id]\n",
    "original_data = merge_texts(original_data)\n",
    "training_set_pos = original_data[['_id','text']].loc[(original_data['_id'].isin(pos_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "    \n",
    "training_set_pos['target']='prevention'\n",
    "training_set_neg = original_data[['_id','text']].loc[(original_data['_id'].isin(neg_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "training_set_neg['target']='not prevention'\n",
    "\n",
    "training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create binary training set using Treatment as an example\n",
    "original_data = read_csv('data/NCT_classification.csv', delimiter=',',header=0)\n",
    "\n",
    "with open('data/behavioral.txt','rb') as dmpfile:\n",
    "    behavioral = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/specificCats.txt','rb') as dompfile:\n",
    "    traindf = pickle.load(dompfile)\n",
    "\n",
    "with open('data/diagnosis.txt','rb') as dmpfile:\n",
    "    diagnosis = pickle.load(dmpfile)  \n",
    "\n",
    "with open('data/prevention.txt','rb') as dmpfile:\n",
    "    prevention = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/treatment.txt','rb') as dmpfile:\n",
    "    treatment = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/drug.txt','rb') as dmpfile:\n",
    "    drug = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/basic science.txt','rb') as dmpfile:\n",
    "    basic_science = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/observational.txt','rb') as dmpfile:\n",
    "    observation = pickle.load(dmpfile)\n",
    "\n",
    "    \n",
    "pos_id = list(set(treatment).union(set(traindf['_id'].loc[((traindf['topicCategory']=='Treatment')|\n",
    "                                                           (traindf['topicCategory']=='Vaccines')|\n",
    "                                                           (traindf['topicCategory']=='Biologics')|\n",
    "                                                           (traindf['topicCategory']=='Medical Care'))].tolist()))-set(basic_science)-set(observation))\n",
    "maybe_neg = list(set(traindf['_id'].loc[((traindf['topicCategory']!='Treatment')&\n",
    "                                         (traindf['topicCategory']!='Vaccines')&\n",
    "                                         (traindf['topicCategory']!='Biologics')&\n",
    "                                         (traindf['topicCategory']!='Medical Care'))].tolist()).union(\n",
    "                 set(prevention).union(set(diagnosis).union(set(basic_science).union(set(observation))))))\n",
    "\n",
    "neg_id = [item  for item in maybe_neg if item not in pos_id]\n",
    "original_data = merge_texts(original_data)\n",
    "training_set_pos = original_data[['_id','text']].loc[(original_data['_id'].isin(pos_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "    \n",
    "training_set_pos['target']='treatment'\n",
    "training_set_neg = original_data[['_id','text']].loc[(original_data['_id'].isin(neg_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "training_set_neg['target']='not treatment'\n",
    "\n",
    "training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/specificCats.txt','rb') as dompfile:\n",
    "    traindf = pickle.load(dompfile)\n",
    "\n",
    "print(len(traindf))\n",
    "print(traindf.groupby('topicCategory').size())\n",
    "print(len(traindf['_id'].loc[((traindf['topicCategory']!='Treatment')&\n",
    "                                         (traindf['topicCategory']!='Vaccines')&\n",
    "                                         (traindf['topicCategory']!='Biologics')&\n",
    "                                         (traindf['topicCategory']!='Medical Care'))]))\n",
    "\n",
    "print(len(traindf['_id'].loc[((traindf['topicCategory']=='Treatment')|\n",
    "                                                           (traindf['topicCategory']=='Vaccines')|\n",
    "                                                           (traindf['topicCategory']=='Biologics')|\n",
    "                                                           (traindf['topicCategory']=='Medical Care'))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_set_pos))\n",
    "print(len(training_set_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Vectorize the text for classifier\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(training_set['text'])\n",
    "features = vectorizer.get_feature_names()\n",
    "print(X.shape)\n",
    "\n",
    "#### Split the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, training_set.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Classify training text as in category or not in category\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "report = classification_report(y_test,y_pred,output_dict=True)\n",
    "print(pd.DataFrame(report))\n",
    "\n",
    "probs = classifier.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "print(auc)\n",
    "print('[[true neg     false pos]]')\n",
    "print('[[false neg     true pos]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand beyond training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run the classifier on the entire training dataset without splitting\n",
    "####Vectorize the training set for classifier\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(training_set['text'])\n",
    "features = vectorizer.get_feature_names()\n",
    "print(X.shape)\n",
    "\n",
    "## Save the vectorizer\n",
    "#vectorizerfile = \"results/models/behavioral_vectorizer.pickle\"\n",
    "#vectorizerfile = \"results/models/diagnosis_vectorizer.pickle\"\n",
    "#vectorizerfile = \"results/models/prevention_vectorizer.pickle\"\n",
    "vectorizerfile = \"results/models/treatment_vectorizer.pickle\"\n",
    "pickle.dump(vectorizer, open(vectorizerfile, \"wb\"))\n",
    "\n",
    "#### train the model on all the data\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=None)\n",
    "classifier.fit(X, training_set.target)\n",
    "\n",
    "## Save the Model\n",
    "#filename = 'results/models/behavioral_randomforest.sav'\n",
    "#filename = 'results/models/diagnosis_randomforest.sav'\n",
    "#filename = 'results/models/prevention_randomforest.sav'\n",
    "filename = 'results/models/treatment_randomforest.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved models\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Apply the model\n",
    "## Pull out all the data \n",
    "idlist = get_source_ids()\n",
    "textdf = batch_fetch_meta(idlist)\n",
    "textdf = merge_texts(textdf)\n",
    "\n",
    "## Remove records with no text\n",
    "nonan = textdf.loc[((~textdf['text'].isna())&\n",
    "                   (textdf['text'].str.len()>5))]\n",
    "\n",
    "print(len(idlist), len(nonan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Vectorize the text based on the previously trained vectorizer and run the classifier\n",
    "labels = nonan['_id']\n",
    "M = vectorizer.transform(nonan['text'])\n",
    "prediction = classifier.predict(M)\n",
    "\n",
    "## Save the results\n",
    "#classifier_results = pd.DataFrame(list(zip(labels, prediction)), columns =['_id', 'behavioral_prediction'])\n",
    "#print(classifier_results.head(n=2))\n",
    "#classifier_results.to_csv('results/predictions/behavioral_randomforest.tsv',\n",
    "#                          sep='\\t', header=True)\n",
    "\n",
    "#classifier_results = pd.DataFrame(list(zip(labels, prediction)), columns =['_id', 'diagnosis_prediction'])\n",
    "#print(classifier_results.head(n=2))\n",
    "#classifier_results.to_csv('results/predictions/diagnosis_randomforest.tsv',\n",
    "#                          sep='\\t', header=True)\n",
    "\n",
    "#classifier_results = pd.DataFrame(list(zip(labels, prediction)), columns =['_id', 'prevention_prediction'])\n",
    "#print(classifier_results.head(n=2))\n",
    "#classifier_results.to_csv('results/predictions/prevention_randomforest.tsv',\n",
    "#                          sep='\\t', header=True)\n",
    "\n",
    "classifier_results = pd.DataFrame(list(zip(labels, prediction)), columns =['_id', 'treatment_prediction'])\n",
    "print(classifier_results.head(n=2))\n",
    "classifier_results.to_csv('results/predictions/treatment_randomforest.tsv',\n",
    "                          sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_results = classifier_results.groupby('treatment_prediction').size().reset_index(name='counts')\n",
    "print(inspect_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
