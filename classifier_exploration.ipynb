{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier tests\n",
    "\n",
    "This script tests the utility of various Scikit learn classifiers towards the task of classifying text for livertox ratings.\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "https://towardsdatascience.com/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca\n",
    "\n",
    "https://stackabuse.com/text-classification-with-python-and-scikit-learn/\n",
    "\n",
    "\n",
    "https://buhrmann.github.io/tfidf-analysis.html\n",
    "\n",
    "https://stackabuse.com/understanding-roc-curves-with-python/\n",
    "\n",
    "https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/\n",
    "\n",
    "about lemmatization https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word\n",
    "https://stackoverflow.com/questions/28475620/wordnet-lemmatizer-in-nltk-is-not-working-for-adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import ipywidgets \n",
    "import widgetsnbextension\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to clean up the text for analysis\n",
    "def clean_up_text(data_set):\n",
    "    data_set['clean_text'] = data_set['section_text'].str.replace(r'\\W', ' ')\n",
    "    data_set['clean_text'] = data_set['clean_text'].str.replace(r'\\s+[a-zA-Z]\\s+', ' ')\n",
    "    data_set['clean_text'] = data_set['clean_text'].str.replace(r'\\^[a-zA-Z]\\s+', ' ')\n",
    "    data_set['clean_text'] = data_set['clean_text'].str.lower()\n",
    "    return(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training and test sets from just the LiverTox corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 166\n"
     ]
    }
   ],
   "source": [
    "#txttype = 'WARNING_PRECAUTION'\n",
    "#txttype = 'OVERDOSE'\n",
    "#txttype = 'ADVERSE REACT'\n",
    "#txttype = 'INTERACTION'\n",
    "#txttype = 'INDICATION'\n",
    "#txttype = 'POPULATION'\n",
    "#txttype = 'WARNBOX'\n",
    "txttype = 'all_toxsxns'\n",
    "\n",
    "livertox_file = read_csv('results/openfda/classifier/livertox_dataset_'+txttype+'.tsv', \n",
    "                         delimiter='\\t',header=0, index_col=0)\n",
    "#print(livertox_file.head(n=2))\n",
    "\n",
    "## Create the binary subset\n",
    "training_set_pos = livertox_file.loc[(livertox_file['likelihood_score']=='A')|\n",
    "                                     (livertox_file['likelihood_score']=='B')|\n",
    "                                     (livertox_file['likelihood_score']=='C')].copy()\n",
    "training_set_pos['target'] = 'livertoxic'\n",
    "training_set_neg = livertox_file.loc[livertox_file['likelihood_score']=='E'].copy()\n",
    "training_set_neg['target'] = 'not livertoxic'\n",
    "\n",
    "training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)\n",
    "print(len(training_set_pos), len(training_set_neg))\n",
    "#print(training_set.head(n=2))\n",
    "#print(len(training_set))\n",
    "\n",
    "#### Clean up the text for analysis\n",
    "training_set = clean_up_text(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Consider lemmatizing the text -- Note, this is a slow step, skip altogether if not needed\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    pos_label = (pos_tag(word_tokenize(word))[0][1][0]).lower()\n",
    "    if pos_label == 'j': \n",
    "        pos_label = 'a' \n",
    "    if pos_label in ['r','a', 's', 'v']: # For adjectives and verbs\n",
    "        return lem.lemmatize(word, pos=pos_label)\n",
    "    else:   # For nouns and everything else as it is the default kwarg\n",
    "        return lem.lemmatize(word)\n",
    "\n",
    "i=0\n",
    "for i in tqdm(range(len(training_set))):\n",
    "    try:\n",
    "        tmp_list = training_set.iloc[i]['clean_text'].split()\n",
    "        cleanlist = [get_wordnet_pos(word) for word in tmp_list]\n",
    "        clean_text = ' '.join(cleanlist)\n",
    "        training_set.iloc[i]['clean_text'] = clean_text\n",
    "    except:\n",
    "        print(i,'splitting failed')\n",
    "    i=i+1\n",
    "    \n",
    "print(training_set.head(n=2))\n",
    "\n",
    "#### The lemmatization does not appear to affect the results in the training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Export the lemmatized text since this step is time-consuming\n",
    "#training_set.to_csv('results/openfda/livertox_expanded_lemmatized_training_data.tsv',sep='\\t',header=True,encoding='UTF-8')\n",
    "\n",
    "#### Import lemmatized dataset\n",
    "training_set = read_csv('results/openfda/livertox_expanded_lemmatized_training_data.tsv',delimiter='\\t',header=0, index_col=0,encoding='UTF-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330, 18439)\n"
     ]
    }
   ],
   "source": [
    "####Vectorize the text for classifier\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(training_set['clean_text'])\n",
    "features = vectorizer.get_feature_names()\n",
    "print(X.shape)\n",
    "\n",
    "#### Split the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, training_set.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Training and Test sets from expanded corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311 438\n",
      "311 438\n"
     ]
    }
   ],
   "source": [
    "#### Use the Expanded Set of all toxicity sections\n",
    "txttype = 'all_toxsxns'\n",
    "\n",
    "livertox_file = read_csv('results/openfda/classifier/livertox_expanded_set_'+txttype+'.tsv', \n",
    "                         delimiter='\\t',header=0, index_col=0)\n",
    "#print(livertox_file.head(n=2))\n",
    "\n",
    "## Create the binary subset\n",
    "training_set_pos = livertox_file.loc[(livertox_file['likelihood_score']=='A')|\n",
    "                                     (livertox_file['likelihood_score']=='B')|\n",
    "                                     (livertox_file['likelihood_score']=='C')|\n",
    "                                     (livertox_file['likelihood_score']=='Y')].copy()\n",
    "training_set_pos['target'] = 'livertoxic'\n",
    "training_set_neg = livertox_file.loc[(livertox_file['likelihood_score']=='E')|\n",
    "                                     (livertox_file['likelihood_score']=='Z')].copy()\n",
    "training_set_neg['target'] = 'not livertoxic'\n",
    "\n",
    "print(len(training_set_pos),len(training_set_neg))\n",
    "\n",
    "training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)\n",
    "print(len(training_set_pos), len(training_set_neg))\n",
    "#print(training_set.head(n=2))\n",
    "#print(len(training_set))\n",
    "\n",
    "#### Clean up the text for analysis\n",
    "training_set = clean_up_text(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(749, 23719)\n"
     ]
    }
   ],
   "source": [
    "####Vectorize the text for classifier\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(training_set['clean_text'])\n",
    "features = vectorizer.get_feature_names()\n",
    "print(X.shape)\n",
    "\n",
    "#### Split the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, training_set.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Training from expanded corpus and LiverTox set, Test set from Livertox only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 166\n",
      "38 38\n",
      "147 272\n",
      "673\n"
     ]
    }
   ],
   "source": [
    "txttype = 'all_toxsxns'\n",
    "\n",
    "livertox_file = read_csv('results/openfda/classifier/livertox_expanded_set_'+txttype+'.tsv', \n",
    "                         delimiter='\\t',header=0, index_col=0)\n",
    "#print(livertox_file.head(n=2))\n",
    "\n",
    "## Create the binary subset\n",
    "training_set_pos = livertox_file.loc[(livertox_file['likelihood_score']=='A')|\n",
    "                                     (livertox_file['likelihood_score']=='B')|\n",
    "                                     (livertox_file['likelihood_score']=='C')].copy()\n",
    "training_set_pos['target'] = 'livertoxic'\n",
    "training_set_neg = livertox_file.loc[livertox_file['likelihood_score']=='E'].copy()\n",
    "training_set_neg['target'] = 'not livertoxic'\n",
    "print(len(training_set_pos),len(training_set_neg))\n",
    "\n",
    "test_set_pos = training_set_pos.sample(frac=0.23,random_state=1)\n",
    "test_set_neg = training_set_neg.sample(frac=0.23,random_state=1)\n",
    "print(len(test_set_pos), len(test_set_neg))\n",
    "\n",
    "expanded_set_pos = livertox_file.loc[livertox_file['likelihood_score']=='Y'].copy()\n",
    "expanded_set_pos['target'] = 'livertoxic'\n",
    "expanded_set_neg = livertox_file.loc[livertox_file['likelihood_score']=='Z'].copy()\n",
    "expanded_set_neg['target'] = 'not livertoxic'\n",
    "print(len(expanded_set_pos),len(expanded_set_neg))\n",
    "\n",
    "total_set = pd.concat((training_set_pos,training_set_neg,expanded_set_pos,expanded_set_neg),ignore_index=True)\n",
    "test_set = pd.concat((test_set_pos,test_set_neg),ignore_index=True)\n",
    "training_set = pd.merge(total_set,test_set, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
    "print(len(training_set))\n",
    "\n",
    "#### Clean up the text for analysis\n",
    "training_set = clean_up_text(training_set)\n",
    "total_set = clean_up_text(total_set)\n",
    "test_set = clean_up_text(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(673, 22421)\n",
      "(76,)\n"
     ]
    }
   ],
   "source": [
    "####Vectorize the text for classifier using the training set and apply to test set\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(training_set['clean_text'])\n",
    "X_test = vectorizer.transform(test_set['clean_text'])\n",
    "features = vectorizer.get_feature_names()\n",
    "y_train = training_set.target\n",
    "y_test = test_set.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "classifier = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=50, max_iter=5, tol=None)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "classifier = MLPClassifier(alpha=1, max_iter=1000)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "classifier = tree.DecisionTreeClassifier(max_depth=5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(3)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel=\"linear\", C=0.025)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "classifier = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "classifier = AdaBoostClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review predictive performance of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  4]\n",
      " [ 6 30]]\n",
      "                   0          1  accuracy  macro avg  weighted avg\n",
      "precision   0.812500   0.882353  0.848485   0.847426      0.850602\n",
      "recall      0.866667   0.833333  0.848485   0.850000      0.848485\n",
      "f1-score    0.838710   0.857143  0.848485   0.847926      0.848764\n",
      "support    30.000000  36.000000  0.848485  66.000000     66.000000\n",
      "0.9296296296296296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "#print(classification_report(y_test,y_pred))\n",
    "#print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test,y_pred,output_dict=True)\n",
    "print(pd.DataFrame(report))\n",
    "#print(report)\n",
    "## Calculate AUC\n",
    "probs = classifier.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classifier comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load the classifiers\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifiers = {\n",
    "    'Random Forest':RandomForestClassifier(n_estimators=1000, random_state=0),\n",
    "    'MultinomialNB':MultinomialNB(),\n",
    "    'SGDClassifier':SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=50, max_iter=5, tol=None),\n",
    "    'Neural Net':MLPClassifier(alpha=1, max_iter=1000),\n",
    "    'Decision Tree':tree.DecisionTreeClassifier(max_depth=5),\n",
    "    'Nearest Neighbor':KNeighborsClassifier(3),\n",
    "    'Linear SVM':SVC(kernel=\"linear\", C=0.025),\n",
    "    'RBF SVM':SVC(gamma=2, C=1),\n",
    "    #'Gaussian Process':GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    'AdaBoost':AdaBoostClassifier(),\n",
    "    #'Naive Bayes':GaussianNB(),\n",
    "    #'QDA':QuadraticDiscriminantAnalysis(),\n",
    "    'Logistic Regression':LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49948ba3817f412d843276916686e3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ginger\\anaconda3\\envs\\pywikibot\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      classifier  CM_0_0  CM_0_1  CM_1_0  CM_1_1  livertoxic_precision  \\\n",
      "0  Random Forest      51       8      12      79              0.809524   \n",
      "1  MultinomialNB      22      37       2      89              0.916667   \n",
      "\n",
      "   livertoxic_recall  livertoxic_f  livertoxic_support  \\\n",
      "0           0.864407      0.836066                  59   \n",
      "1           0.372881      0.530120                  59   \n",
      "\n",
      "   not_livertoxic_precision  ...  macro_avg_precision  macro_avg_recall  \\\n",
      "0                  0.908046  ...             0.858785          0.866269   \n",
      "1                  0.706349  ...             0.811508          0.675452   \n",
      "\n",
      "   macro_avg_f  macro_avg_support  wt_avg_precision  wt_avg_recall  wt_avg_f  \\\n",
      "0     0.861853                150          0.869294       0.866667  0.867354   \n",
      "1     0.675198                150          0.789074       0.740000  0.706148   \n",
      "\n",
      "   wt_avg_support  accuracy       AUC  \n",
      "0             150  0.866667  0.941237  \n",
      "1             150  0.740000  0.891786  \n",
      "\n",
      "[2 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "#### Loop through them \n",
    "classifiers_list = list(classifiers.keys())\n",
    "classifier_results_list = []\n",
    "\n",
    "for i in tqdm(range(len(classifiers_list))):\n",
    "    classifier = classifiers[classifiers_list[i]]\n",
    "    ## Train via classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    ## Test classifier\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    ## Get Metrics\n",
    "    report = classification_report(y_test,y_pred,output_dict=True)\n",
    "    ## Calculate AUC\n",
    "    try:\n",
    "        probs = classifier.predict_proba(X_test)\n",
    "        probs = probs[:, 1]\n",
    "        auc = roc_auc_score(y_test, probs)\n",
    "    except:\n",
    "        auc = 'not calculated'\n",
    "    ## Save metrics\n",
    "    metrics_dict = {'classifier':classifiers_list[i],\n",
    "                    'CM_0_0':confusion_matrix(y_test,y_pred)[0][0],\n",
    "                    'CM_0_1':confusion_matrix(y_test,y_pred)[0][1],\n",
    "                    'CM_1_0':confusion_matrix(y_test,y_pred)[1][0],\n",
    "                    'CM_1_1':confusion_matrix(y_test,y_pred)[1][1],\n",
    "                    'livertoxic_precision':report['livertoxic']['precision'],\n",
    "                    'livertoxic_recall':report['livertoxic']['recall'],\n",
    "                    'livertoxic_f':report['livertoxic']['f1-score'],\n",
    "                    'livertoxic_support':report['livertoxic']['support'],\n",
    "                    'not_livertoxic_precision':report['not livertoxic']['precision'],\n",
    "                    'not_livertoxic_recall':report['not livertoxic']['recall'],\n",
    "                    'not_livertoxic_f':report['not livertoxic']['f1-score'],\n",
    "                    'not_livertoxic_support':report['not livertoxic']['support'],\n",
    "                    'macro_avg_precision':report['macro avg']['precision'],\n",
    "                    'macro_avg_recall':report['macro avg']['recall'],\n",
    "                    'macro_avg_f':report['macro avg']['f1-score'],\n",
    "                    'macro_avg_support':report['macro avg']['support'],   \n",
    "                    'wt_avg_precision':report['weighted avg']['precision'],\n",
    "                    'wt_avg_recall':report['weighted avg']['recall'],\n",
    "                    'wt_avg_f':report['weighted avg']['f1-score'],\n",
    "                    'wt_avg_support':report['weighted avg']['support'],\n",
    "                    'accuracy':report['accuracy'],\n",
    "                    'AUC':auc\n",
    "                   }\n",
    "    classifier_results_list.append(metrics_dict)\n",
    "    \n",
    "\n",
    "classifier_results_df = pd.DataFrame(classifier_results_list)\n",
    "print(classifier_results_df.head(n=2))\n",
    "classifier_results_df.to_csv('results/openfda/classifier_results_df.tsv',sep='\\t',header=True,encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the classifier to the LiverTox dataset and predict if an SPL is livertoxic or not\n",
    "\n",
    "https://levelup.gitconnected.com/scikit-learn-machine-learning-classification-101-c431de2dc2b2\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/make-predictions-scikit-learn/\n",
    "\n",
    "https://machinelearningmastery.com/train-final-machine-learning-model/\n",
    "\n",
    "https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(749, 23719)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9653bd674c4a3ca464c2f2e012b9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Verify performance of top 2 classifiers using K-folds cross validator\n",
    "from sklearn.model_selection import KFold\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(training_set['clean_text'])\n",
    "y = training_set.target\n",
    "features = vectorizer.get_feature_names()\n",
    "print(X.shape)\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "\n",
    "classifier_results_list =[]\n",
    "classifiers = {\n",
    "    'Random Forest':RandomForestClassifier(n_estimators=1000, random_state=0),\n",
    "    'AdaBoost':AdaBoostClassifier()}\n",
    "classifiers_list = list(classifiers.keys())\n",
    "\n",
    "for i in tqdm(range(len(classifiers_list))):\n",
    "    classifier = classifiers[classifiers_list[i]]\n",
    "    classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        ## Train via classifier\n",
    "        classifier.fit(X_train, y_train)\n",
    "        ## Test classifier\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        ## Get Metrics\n",
    "        report = classification_report(y_test,y_pred,output_dict=True)\n",
    "        ## Calculate AUC\n",
    "        try:\n",
    "            probs = classifier.predict_proba(X_test)\n",
    "            probs = probs[:, 1]\n",
    "            auc = roc_auc_score(y_test, probs)\n",
    "        except:\n",
    "            auc = 'not calculated'\n",
    "        ## Save metrics\n",
    "        metrics_dict = {'classifier':classifiers_list[i],\n",
    "                        'CM_0_0':confusion_matrix(y_test,y_pred)[0][0],\n",
    "                        'CM_0_1':confusion_matrix(y_test,y_pred)[0][1],\n",
    "                        'CM_1_0':confusion_matrix(y_test,y_pred)[1][0],\n",
    "                        'CM_1_1':confusion_matrix(y_test,y_pred)[1][1],\n",
    "                        'livertoxic_precision':report['livertoxic']['precision'],\n",
    "                        'livertoxic_recall':report['livertoxic']['recall'],\n",
    "                        'livertoxic_f':report['livertoxic']['f1-score'],\n",
    "                        'livertoxic_support':report['livertoxic']['support'],\n",
    "                        'not_livertoxic_precision':report['not livertoxic']['precision'],\n",
    "                        'not_livertoxic_recall':report['not livertoxic']['recall'],\n",
    "                        'not_livertoxic_f':report['not livertoxic']['f1-score'],\n",
    "                        'not_livertoxic_support':report['not livertoxic']['support'],\n",
    "                        'macro_avg_precision':report['macro avg']['precision'],\n",
    "                        'macro_avg_recall':report['macro avg']['recall'],\n",
    "                        'macro_avg_f':report['macro avg']['f1-score'],\n",
    "                        'macro_avg_support':report['macro avg']['support'],   \n",
    "                        'wt_avg_precision':report['weighted avg']['precision'],\n",
    "                        'wt_avg_recall':report['weighted avg']['recall'],\n",
    "                        'wt_avg_f':report['weighted avg']['f1-score'],\n",
    "                        'wt_avg_support':report['weighted avg']['support'],\n",
    "                        'accuracy':report['accuracy'],\n",
    "                        'AUC':auc\n",
    "                       }\n",
    "        classifier_results_list.append(metrics_dict)\n",
    "\n",
    "classifier_results_df = pd.DataFrame(classifier_results_list)\n",
    "classifier_results_df.to_csv('results/openfda/top_two_classifier_cross_validation_results_df.tsv',sep='\\t',header=True,encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(749, 23719)\n"
     ]
    }
   ],
   "source": [
    "#### Run the classifier on the entire training dataset without splitting\n",
    "####Vectorize the training set for classifier\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(training_set['clean_text'])\n",
    "features = vectorizer.get_feature_names()\n",
    "print(X.shape)\n",
    "\n",
    "## Save the vectorizer\n",
    "vectorizerfile = \"results/openfda/classifier/models_vectorizer.pickle\"\n",
    "pickle.dump(vectorizer, open(vectorizerfile, \"wb\"))\n",
    "\n",
    "#### train the model on all the data\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=None)\n",
    "classifier.fit(X, training_set.target)\n",
    "\n",
    "## Save the Model\n",
    "filename = 'results/openfda/classifier/models_randomforest.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved models\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  active_code livertox_prediction\n",
      "0  2RQ1L9N089      not livertoxic\n",
      "1  PDC6A3C0OX      not livertoxic\n"
     ]
    }
   ],
   "source": [
    "#### Apply the model\n",
    "\n",
    "## Pull out all the data \n",
    "spl_alltox = read_csv('results/openfda/all_toxsxns_sample_text_per_code.tsv', delimiter='\\t',header=0, index_col=0)\n",
    "#print(spl_alltox.head(n=2))\n",
    "\n",
    "## Remove active_codes with no text\n",
    "nonan = spl_alltox.loc[~spl_alltox['section_text'].isna()]\n",
    "#print(len(spl_alltox), len(nonan))\n",
    "\n",
    "## Vectorize the text based on the previously trained vectorizer and run the classifier\n",
    "labels = nonan['active_code']\n",
    "M = vectorizer.transform(nonan['section_text'])\n",
    "prediction = classifier.predict(M)\n",
    "\n",
    "## Save the results\n",
    "classifier_results = pd.DataFrame(list(zip(labels, prediction)), columns =['active_code', 'livertox_prediction'])\n",
    "print(classifier_results.head(n=2))\n",
    "classifier_results.to_csv('results/openfda/classifier/livertox_predictions_all_toxsxns_randomforest.tsv',\n",
    "                          sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Check the model againt the original data\n",
    "livertox_scores = training_set[['active_code','likelihood_score','target']]\n",
    "#classifier_results_check = classifier_results.merge(livertox_scores, on='active_code', how='inner')\n",
    "#classifier_failures = classifier_results_check.loc[classifier_results_check['livertox_prediction']!=classifier_results_check['target']]\n",
    "#print(classifier_failures)\n",
    "\n",
    "classifier_results_check = classifier_results.merge(livertox_scores, on='active_code', how='left')\n",
    "classifier_results_check.to_csv('results/openfda/classifier/livertox_predictions_all_toxsxns_randomforest_chk.tsv',\n",
    "                          sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1195\n",
      "  active_code livertox_prediction likelihood_score target\n",
      "0  2RQ1L9N089      not livertoxic              NaN    NaN\n",
      "1  PDC6A3C0OX      not livertoxic              NaN    NaN\n"
     ]
    }
   ],
   "source": [
    "classifier_results_check = read_csv('results/openfda/classifier/livertox_predictions_all_toxsxns_randomforest_chk.tsv',\n",
    "                          delimiter='\\t', header=0, index_col=0)\n",
    "#print(classifier_results_check.head(n=2))\n",
    "\n",
    "not_in_wd_or_livertox = classifier_results_check.loc[(classifier_results_check['likelihood_score'].isna())|\n",
    "                                                     (classifier_results_check['likelihood_score']=='Y')|\n",
    "                                                     (classifier_results_check['likelihood_score']=='Z')]\n",
    "print(len(not_in_wd_or_livertox))\n",
    "print(not_in_wd_or_livertox.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QC the results\n",
    "## In WD, map UNIIs to drug classes with ratings in Livertox\n",
    "## Apply drug class livertox ratings to individual drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Check FDA SPLs for non-compliance\n",
    "\n",
    "## Get spls from drugs with livertoxicity\n",
    "\n",
    "## pull/merge sections for each drug and run classifier\n",
    "\n",
    "## Look for spls which are classified as not toxic for drugs where active ingredient is toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## save plot\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "probs = classifier.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "fpr, tpr, thresholds = roc_curve(testy, probs)\n",
    "plot_roc_curve(fpr, tpr)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
