{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for fetching relevant metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull all Clinical Trials data \n",
    "\n",
    "def fetch_src_size():\n",
    "    pubmeta = requests.get(\"https://api.outbreak.info/resources/query?q=@type:ClinicalTrial&size=0&aggs=@type\")\n",
    "    pubjson = json.loads(pubmeta.text)\n",
    "    pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "    return(pubcount)\n",
    "\n",
    "\n",
    "#### Pull ids from a json file\n",
    "def get_ids_from_json(jsonfile):\n",
    "    idlist = []\n",
    "    for eachhit in jsonfile[\"hits\"]:\n",
    "        if eachhit[\"_id\"] not in idlist:\n",
    "            idlist.append(eachhit[\"_id\"])\n",
    "    return(idlist)\n",
    "\n",
    "\n",
    "#### Ping the API and get all the ids for a clinical trials and scroll through the source until number of ids matches meta\n",
    "def get_source_ids():\n",
    "    source_size = fetch_src_size()\n",
    "    r = requests.get(\"https://api.outbreak.info/resources/resource/query?q=@type:ClinicalTrial&fields=_id&fetch_all=true\")\n",
    "    response = json.loads(r.text)\n",
    "    idlist = get_ids_from_json(response)\n",
    "    try:\n",
    "        scroll_id = response[\"_scroll_id\"]\n",
    "        while len(idlist) < source_size:\n",
    "            r2 = requests.get(\"https://api.outbreak.info/resources/resource/query?q=@type:ClinicalTrial&fields=_id&fetch_all=true&scroll_id=\"+scroll_id)\n",
    "            response2 = json.loads(r2.text)\n",
    "            idlist2 = set(get_ids_from_json(response2))\n",
    "            tmpset = set(idlist)\n",
    "            idlist = list(tmpset.union(idlist2))\n",
    "            try:\n",
    "                scroll_id = response2[\"_scroll_id\"]\n",
    "            except:\n",
    "                print(\"no new scroll id\")\n",
    "        return(idlist)\n",
    "    except:\n",
    "        return(idlist)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the key metadata for all clinical trials\n",
    "#### Get the metadata for each list\n",
    "#### Note, I've tried batches of 1000, and the post request has failed, so this uses a batch size that's less likely to fail\n",
    "def batch_fetch_clin_meta(idlist):\n",
    "    ## Break the list of ids into smaller chunks so the API doesn't fail the post request\n",
    "    runs = round((len(idlist))/100,0)\n",
    "    i=0 \n",
    "    separator = ','\n",
    "    ## Create dummy dataframe to store the meta data\n",
    "    textdf = pd.DataFrame(columns = ['_id','abstract','trialName','trialDescription',\n",
    "                                     'designPrimaryPurpose','studyType',\n",
    "                                     'interventionCategory','interventionName'])\n",
    "    while i < runs+1:\n",
    "        if len(idlist)<100:\n",
    "            sample = idlist\n",
    "        elif i == 0:\n",
    "            sample = idlist[i:(i+1)*100]\n",
    "        elif i == runs:\n",
    "            sample = idlist[i*100:len(idlist)]\n",
    "        else:\n",
    "            sample = idlist[i*100:(i+1)*100]\n",
    "        sample_ids = separator.join(sample)\n",
    "        ## Get the text-based metadata (abstract, title) and save it\n",
    "        r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': sample_ids, 'scopes': '_id', 'fields': 'name,abstract,description,interventions,studyDesign'})\n",
    "        if r.status_code == 200:\n",
    "            rawresult = json.loads(r.text)\n",
    "            structuredresult = pd.json_normalize(rawresult)\n",
    "            structuredresult.drop(columns=['studyDesign.@type','studyDesign.designModel',\n",
    "                                           'studyDesign.phaseNumber','studyDesign.phase',\n",
    "                                           'studyDesign.designAllocation','studyDesign.studyDesignText'],inplace=True)\n",
    "            structuredresult.rename(columns={'name':'trialName', 'description':'trialDescription',\n",
    "                                             'studyDesign.designPrimaryPurpose':'designPrimaryPurpose',\n",
    "                                             'studyDesign.studyType':'studyType'},inplace=True)\n",
    "            exploded = structuredresult.explode('interventions')\n",
    "            no_interventions = exploded.loc[exploded['interventions'].isna()].copy()\n",
    "            no_interventions_clean = no_interventions[['_id','abstract','trialName','trialDescription',\n",
    "                                                       'designPrimaryPurpose','studyType']].copy()\n",
    "            has_interventions = exploded.loc[~exploded['interventions'].isna()].copy()\n",
    "            interventions = pd.concat([has_interventions.drop(['interventions'], axis=1), has_interventions['interventions'].apply(pd.Series)], axis=1)\n",
    "            clean_interventions = interventions[['_id','abstract','trialName','trialDescription',\n",
    "                                                 'designPrimaryPurpose','studyType',\n",
    "                                                 'category','name']].copy()\n",
    "            clean_interventions.rename(columns={'name':'interventionName','category':'interventionCategory'},inplace=True)\n",
    "            textdf = pd.concat((textdf,clean_interventions,no_interventions_clean),ignore_index=True)\n",
    "        i=i+1\n",
    "    textdf.rename(columns={'trialName':'name','trialDescription':'description'},inplace=True)\n",
    "    return(textdf)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for transforming the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merge text from the name, abstract, and description\n",
    "#### Clean up up the text\n",
    "\n",
    "def merge_texts(df):\n",
    "    df.fillna('',inplace=True)\n",
    "    df['text'] = df['name'].astype(str).str.cat(df['abstract'].astype(str).str.cat(df['description'],sep=' '),sep=' ')\n",
    "    df['text'] = df['text'].str.replace(r'\\W', ' ')\n",
    "    df['text'] = df['text'].str.replace(r'\\s+[a-zA-Z]\\s+', ' ')\n",
    "    df['text'] = df['text'].str.replace(r'\\^[a-zA-Z]\\s+', ' ')\n",
    "    df['text'] = df['text'].str.lower()   \n",
    "    return(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_df(pos_id,maybe_neg_id,original_data):\n",
    "    neg_id = [item  for item in maybe_neg if item not in pos_id]\n",
    "    original_data = merge_texts(original_data)\n",
    "    training_set_pos = original_data[['_id','text']].loc[(original_data['_id'].isin(pos_id))&\n",
    "                                                         (~original_data['text'].isna())]\n",
    "    training_set_pos['target']='in category'\n",
    "    training_set_neg = original_data[['_id','text']].loc[(original_data['_id'].isin(neg_id))&\n",
    "                                                         (~original_data['text'].isna())]\n",
    "    training_set_neg['target']='not in category'\n",
    "    training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)\n",
    "    return(training_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9918\n",
      "Wall time: 8.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "idlist = get_source_ids()\n",
    "print(len(idlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14811\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clin_meta = batch_fetch_clin_meta(idlist)\n",
    "print(len(clin_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clin_meta = merge_texts(clin_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split clinical trials up into specific topicCategories based on available Meta data\n",
    "The clinical trials meta contains enough information to readily classify the records into broad topicCategories, but requires more work in order to map the data to specific topicCategories\n",
    "Fields which contain valuable sorting information include:\n",
    "1. designPrimaryPurpose\n",
    "2. intervention category\n",
    "3. intervention name (but this one is highly variable, and will need a lot of work to map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping the records based on designPrimaryPurpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 designPrimaryPurpose  counts\n",
      "0                                                        7623\n",
      "22                                          treatment    4342\n",
      "13                                         prevention    1315\n",
      "12                                              other     363\n",
      "20                                    supportive care     331\n",
      "5                                          diagnostic     249\n",
      "10                           health services research     248\n",
      "2                                       basic science     104\n",
      "19                                          screening      91\n",
      "17                                          prognosis      46\n",
      "11                                    natural history      22\n",
      "1                  basic research/physiological study      19\n",
      "6                                  education/guidance      14\n",
      "3                                  device feasibility      10\n",
      "18                                       psychosocial       9\n",
      "9                             health service research       8\n",
      "7                educational / counselling / training       5\n",
      "24  treatment, randomization description: block ra...       2\n",
      "25  treatment, randomization description: patients...       1\n",
      "26  treatment, randomization description: the samp...       1\n",
      "23  treatment, other design features: no, randomiz...       1\n",
      "14  prevention, randomization description: critica...       1\n",
      "21  supportive, randomization description: method ...       1\n",
      "16  prevention, randomization description: using a...       1\n",
      "15  prevention, randomization description: randomi...       1\n",
      "8                                    health economics       1\n",
      "4                                           diagnosis       1\n",
      "27  treatment, randomization description: the sele...       1\n"
     ]
    }
   ],
   "source": [
    "#print(clin_meta['designPrimaryPurpose'].unique().tolist())\n",
    "designpurpose = clin_meta.groupby('designPrimaryPurpose').size().reset_index(name='counts')\n",
    "designpurpose.sort_values('counts',ascending=False,inplace=True)\n",
    "print(designpurpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categories above may reasonably be mapped as follows:\n",
    "* treatment: Treatment\n",
    "* prevention: Prevention\n",
    "* ~other~\n",
    "* supportive care: Medical Care, Behavioral Research\n",
    "* diagnostic: Diagnosis\n",
    "* health services research: Medical Care\n",
    "* ~basic science~\n",
    "* screening: Diagnosis\n",
    "* prognosis: Prognosis (should be a subcategory for Treatment)\n",
    "* natural history: Case Descriptions\n",
    "* ~basic research/physiological study~\n",
    "* education/guidance: Behavioral Research\n",
    "* ~device feasibility~\n",
    "* psychosocial: Behavioral Research\n",
    "* ~health service research~\n",
    "* ~educational / counselling / training~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              _id                                           abstract  \\\n",
      "1055  NCT04710316  The new coronavirus known as SARS-Cov-2 (sever...   \n",
      "1056  NCT04710316  The new coronavirus known as SARS-Cov-2 (sever...   \n",
      "\n",
      "                                                   name description  \\\n",
      "1055  Étude de l'épidémie de SARS-CoV-2 Dans Les Ser...               \n",
      "1056  Étude de l'épidémie de SARS-CoV-2 Dans Les Ser...               \n",
      "\n",
      "     designPrimaryPurpose       studyType interventionCategory  \\\n",
      "1055            screening  interventional      diagnostic test   \n",
      "1056            screening  interventional      diagnostic test   \n",
      "\n",
      "                               interventionName  \\\n",
      "1055  SARS-CoV-2 screening by molecular biology   \n",
      "1056                      Serological screening   \n",
      "\n",
      "                                                   text  \n",
      "1055  étude de épidémie de sars cov 2 dans les servi...  \n",
      "1056  étude de épidémie de sars cov 2 dans les servi...  \n"
     ]
    }
   ],
   "source": [
    "print(clin_meta.loc[clin_meta['designPrimaryPurpose']=='screening'].head(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the records which cannot be mapped based on designPrimaryPurpose\n",
    "Check if they have an intervention category. If they don't have any of these, they are candidates to be classified by the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/'\n",
    "nopurpose = clin_meta.loc[clin_meta['designPrimaryPurpose']==\"\"]\n",
    "## Note, the interventionCategory is used to identify drugs and supplements downstream\n",
    "## Hence, even if some entries have drugs under interventionName, they may be neglected \n",
    "no_int = nopurpose.loc[nopurpose['interventionCategory']==\"\"]\n",
    "with open(os.path.join(DATAPATH,'blank_entries.pickle'),'wb') as uncategorized:\n",
    "    pickle.dump(no_int,uncategorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping the records based on intervention category\n",
    "The intervention categories are primarily used only in NCT records and generally fall under the following categories which can potentially be mapped as follows:\n",
    "* 'biological': Biologics\n",
    "* 'diagnostic test': Diagnosis\n",
    "    * Note that the Intervention name can give insight into subcategories for Diagnosis\n",
    "* 'drug': Treatment\n",
    "    * This can be subdivided into Pharmaceutical Treatments or Repurposing\n",
    "    * To determine which, query the drug in Wikidata and if it has an NDF-RT ID it's FDA approved, therefore Repurpose\n",
    "    * Note that NDF-RT is obsolete, so if it was FDA approved after the fact, it will not appear as such\n",
    "* 'behavioral': Behavioral Research\n",
    "* ~'other'~\n",
    "* ~'combination product'~ (some standard of care, some pharmaceutical\n",
    "* ~'device'~\n",
    "* 'procedure': Medical Care\n",
    "* 'dietary supplement': Repurposing\n",
    "* 'radiation': Medical Care\n",
    "* 'genetic': Host Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate keyword lists based on term frequencies for interventionName in various categories for subclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      interventionName  counts\n",
      "287                                    Lung ultrasound       8\n",
      "318                                Nasopharyngeal swab       8\n",
      "15                             Anti-SARS-CoV2 Serology       4\n",
      "87                                     COVID-19 RT-PCR       4\n",
      "47                                        Blood sample       4\n",
      "..                                                 ...     ...\n",
      "234                               Heterologous stimuli       1\n",
      "235  Home Sleep Apnea Testing or In-hospital Polyso...       1\n",
      "236                  Hospital Anxiety Depression Scale       1\n",
      "237                           Human biological samples       1\n",
      "672                              visual analogue scale       1\n",
      "\n",
      "[673 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "## Diagnostics subclassifiecation\n",
    "diagnostics = clin_meta[['_id','interventionName']].loc[((clin_meta['interventionCategory'].astype(str).str.contains('diagnostic test'))|\n",
    "                                                         (clin_meta['designPrimaryPurpose'].astype(str).str.contains('screening'))|\n",
    "                                                         (clin_meta['designPrimaryPurpose'].astype(str).str.contains('diagnosis')))]\n",
    "diag_word_freq = diagnostics.groupby('interventionName').size().reset_index(name='counts')\n",
    "diag_word_freq.sort_values('counts',ascending=False,inplace=True)\n",
    "print(diag_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        interventionName  counts\n",
      "581              Placebo     116\n",
      "361   Hydroxychloroquine      27\n",
      "604  Placebo oral tablet      11\n",
      "893            mRNA-1273       8\n"
     ]
    }
   ],
   "source": [
    "## Prevention subclassification\n",
    "prevention = clin_meta.loc[clin_meta['designPrimaryPurpose']=='prevention']\n",
    "prev_word_freq = prevention.groupby('interventionName').size().reset_index(name='counts')\n",
    "prev_word_freq.sort_values('counts',ascending=False,inplace=True)\n",
    "print(prev_word_freq.head(n=4))\n",
    "#print(prev_word_freq['interventionName'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create the maps and search terms\n",
    "\n",
    "intervention_map={'genetic':'Host Factors',\n",
    "                  'biological':'Biologics',\n",
    "                  'behavioral':'Behavioral Research',\n",
    "                  'radiation':'Medical Care',\n",
    "                  'procedure': 'Medical Care',\n",
    "                  'dietary supplement': 'Repurposing',\n",
    "                  'diagnostic test': 'Diagnosis'}\n",
    "\n",
    "\n",
    "diagnostickeywords = {'Pathology/Radiology':['graphy','ultrasound','ECG','Pulmonary Function Test','Spirometry','biopsy'],\n",
    "                      'Rapid Diagnostics':['rapid','Rapid'],\n",
    "                      'Virus Detection':['RT-PCR','PCR'],\n",
    "                      #'Antibody Detection':['IgG','IgM','IgE','IgA','antibod','Antibod','antigen','Antigen','ELISA','ELISPOT'],\n",
    "                      'Antibody Detection':['antibod','Antibod','antigen','Anti-SARS-CoV2','Antigen','ELISA','ELISPOT'],\n",
    "                      'Symptoms':['symptom','clinical sign','presenting with','clinical presentation']}\n",
    "\n",
    "treatmentkeywords = {'Vaccines':['vaccin','Vaccin','inactivated virus'],\n",
    "                     'Medical Care':['Ventilat','ventilat','standard of care','soc','s.o.c.']}\n",
    "\n",
    "preventionkeywords = {'Public Health Interventions':['policy','travel restriction','lockdown','quarantine','campaign','closures'],\n",
    "                      'Individual Prevention':['counsel','training','education','awareness','PPE','face mask','face covering','device'],\n",
    "                      'Vaccines':['vaccin','Vaccin','inactivated virus']}\n",
    "\n",
    "designpurposemap = {'treatment': 'Treatment',\n",
    "                    'prevention': 'Prevention',\n",
    "                    'diagnostic': 'Diagnosis',\n",
    "                    'health services research': 'Medical Care',\n",
    "                    'screening': 'Diagnosis',\n",
    "                    'natural history': 'Case Descriptions',\n",
    "                    'education/guidance': 'Behavioral Research',\n",
    "                    'psychosocial': 'Behavioral Research'}\n",
    "\n",
    "#### Potential use of combinations to describe subcategories\n",
    "combi_cats = {\"Individual Prevention\":{'designPrimaryPurpose':'prevention','interventionCategory':'device'}}\n",
    "\n",
    "#### Potential use of single cats to describe combi cats\n",
    "multi_cats = {'supportive care': ['Medical Care','Behavioral Research']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the drugs for classification\n",
    "Sort drugs as Repurposed (if FDA approved), or pharmaceutical treatment (if it's a chemical compound but not a medicine or pharmaceutical drug)\n",
    "\n",
    "In Wikidata:\n",
    "* Chemical compound: Q11173\n",
    "* medication: Q12140\n",
    "* essential medicine: Q35456\n",
    "* pharmaceutical product: Q28885102\n",
    "* drug: Q8386\n",
    "\n",
    "Instances of medication, essential medicine, and pharmaceutical product will be classified as Repurpose\n",
    "Instances of Chemical compound not classified as these others will be considered pharmaceutical treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove stopwords and other text that may cause issues when searching Wikidata results\n",
    "drug_stopwords = {\" Oral Tablet\":\"\",\n",
    "                  \" oral tablet\":\"\",\n",
    "                  \" oral capsule\":\"\",\n",
    "                  \" Oral Product\":\"\",\n",
    "                  \" For Injection\":\"\",\n",
    "                  \" Administration\":\"\",\n",
    "                  \" Nasal Spray and Gargle\":\"\",\n",
    "                  \" Inhalation Solution\":\"\",\n",
    "                  \" Injectable Solution\":\"\",\n",
    "                  \"  - Weekly Dosing\":\"\",\n",
    "                  \"Single Dose of \":\"\",\n",
    "                  \" twice a day\":\"\",\n",
    "                  \" Regular dose\":\"\",\n",
    "                  \" Film Tablets\":\"\"}\n",
    "general_stopwords = {\" Tablet\":\"\",\n",
    "                     \" tablet\":\"\",\n",
    "                     \" inhalation\":\"\",\n",
    "                     \" intravenous\":\"\",\n",
    "                     \" injection\":\"\",\n",
    "                     \" Injection\":\"\",\n",
    "                     \" pill\":\"\",\n",
    "                     \" gas\":\"\",\n",
    "                     \" comparator\":\"\"}\n",
    "pharma_amounts = r\"((?:\\d{1,3}|0\\.\\d{1,3})\\s(?:(?:MG/ML)|(?:mg/mL)|mg|MG|Mg))\"\n",
    "odd_fractions = r\"(/((?:\\d{1,3}|0\\.\\d{1,3})\\s(?:mL|ML|Ml)|(?:KG|kg)))\"\n",
    "\n",
    "#print(clin_meta['interventionCategory'].unique().tolist())\n",
    "\n",
    "drugs = clin_meta[['_id','interventionName','text']].loc[clin_meta['interventionCategory'].astype(str).str.contains('drug')].copy()\n",
    "drugs['interventionName'] = drugs['interventionName'].replace(drug_stopwords,regex=True)\n",
    "drugs['interventionName'] = drugs['interventionName'].replace(general_stopwords,regex=True)\n",
    "drugs['interventionName'] = drugs['interventionName'].str.replace(pharma_amounts,\"\",regex=True)\n",
    "drugs['interventionName'] = drugs['interventionName'].str.replace(odd_fractions,\"\",regex=True)\n",
    "drugs['interventionName'] = drugs['interventionName'].str.replace(' / ','/')\n",
    "drugs['interventionName'] = drugs['interventionName'].str.strip()\n",
    "drug_word_freq = drugs.groupby('interventionName').size().reset_index(name='counts')\n",
    "drug_word_freq.sort_values('counts',ascending=False,inplace=True)\n",
    "drugfreq = drug_word_freq.loc[drug_word_freq['counts']>1].copy()\n",
    "druglist = drugfreq['interventionName'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(druglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wikidata(data):\n",
    "    tmplist = []\n",
    "    for item in data['results']['bindings']:\n",
    "        try:\n",
    "            tmplist.append(OrderedDict({\n",
    "            'wdid':item['item']['value'].replace('http://www.wikidata.org/entity/',''),\n",
    "            'drug_name': item['itemLabel']['value'],\n",
    "            'name': item['itemLabel']['value'].lower(),\n",
    "            'alias': \"None\"}))\n",
    "            tmp= item['itemAltLabel']['value'].split(',')\n",
    "            for altname in tmp:\n",
    "                if len(altname.strip())>3:\n",
    "                    tmplist.append(OrderedDict({\n",
    "                    'wdid':item['item']['value'].replace('http://www.wikidata.org/entity/',''),\n",
    "                    'drug_name': item['itemLabel']['value'],\n",
    "                    'name': item['itemLabel']['value'].lower(),\n",
    "                    'alias': altname.strip().lower()\n",
    "                    }))\n",
    "        except:\n",
    "            tmplist.append(OrderedDict({\n",
    "            'wdid':item['item']['value'].replace('http://www.wikidata.org/entity/',''),\n",
    "            'drug_name': item['itemLabel']['value'],\n",
    "            'name': item['itemLabel']['value'].lower(),\n",
    "            'alias': \"None\"\n",
    "            }))\n",
    "    tmpdf = pd.DataFrame(tmplist)\n",
    "    return(tmpdf)\n",
    "        \n",
    "def get_wd_drugs(): \n",
    "    repurposetypes = ['Q12140', 'Q35456', 'Q28885102','Q8386']\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    querystart = \"\"\"\n",
    "    SELECT\n",
    "      ?item ?itemLabel ?itemAltLabel\n",
    "      ?value \n",
    "    WHERE \n",
    "    {\n",
    "      ?item wdt:P31 wd:\"\"\"\n",
    "    queryend = \"\"\".        \n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "    }\n",
    "    \"\"\"\n",
    "    repurpose = pd.DataFrame(columns=['wdid','drug_name','name','alias'])\n",
    "    for eachwdid in repurposetypes:\n",
    "        query = querystart+eachwdid+queryend\n",
    "        r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "        data = r.json()\n",
    "        tmpdf = parse_wikidata(data)\n",
    "        repurpose = pd.concat((repurpose,tmpdf),ignore_index=True)\n",
    "    repurpose.drop_duplicates(keep='first',inplace=True)\n",
    "    return(repurpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that originally, the script also did a Wikidata query for instance of chemical compounds, but there are so many in Wikidata that the query will time out. However, if a clinical trial is classified as a drug trial, it should generally fall under either repurposing or pharmaceutical intervention depending on whether or not the drug is novel. For this reason, we will define repurposing based on matches in Wikidata and define pharmaceutical based on exclusion from repurposing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Batch fetch names, aliases, and wdid's from Wikidata for medications, pharma products, etc.3\n",
    "repurposedf = get_wd_drugs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30623\n",
      "   wdid drug_name     name              alias\n",
      "0  Q153   ethanol  ethanol               None\n",
      "1  Q153   ethanol  ethanol             spirit\n",
      "2  Q153   ethanol  ethanol             tecsol\n",
      "3  Q153   ethanol  ethanol            alcohol\n",
      "4  Q153   ethanol  ethanol  denatured alcohol\n"
     ]
    }
   ],
   "source": [
    "print(len(repurposedf))\n",
    "print(repurposedf.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['placebo', 'remdesivir', 'hydroxychloroquine sulfate', 'placebos', 'standard of care', 'standard of care', 'camostat mesilate', 'clazakizumab', 'das181', 'standard medical treatment', 'convalescent plasma', 'standard treatment', 'placebo', 'hydroxychloroquine (hcq)', 'opaganib', 'oxygen', 'zinc', 'hcq', 'normal saline', 'dwrx2003', 'colchicines', 'ly3819253', 'corticosteroid', 'rls-0071', 'mavrilimumab', 'regn10933+regn10987 combination therapy', 'placebo (normal saline solution)', 'sng001', 'ivermectins', 'leronlimab (700mg)', 'nitric oxide gas', 'cannabidiol', 'normal saline', 'soc', 'ace inhibitor', 'apixaban 2.', 'chloroquine or hydroxychloroquine', 'control', 'suspension of heat killed (autoclaved) mycobacterium w', 'slv213', 'vitamin d', 'eidd-2801', 'brii-196 and brii-198', 'fisetin', 'proxalutamide', 'pulmozyme', 'azd7442', 'hzvsf-v13', 'snpp protoporphyrin plus sunlight exposure', 'bamlanivimab', 'bcg vaccine', 'lopinavir/ ritonavir', 'selinexor', 'quercetin', 'aerosolized 13 cis retinoic acid', 'ascorbic acid', 'silymarin', 'cyt107', 'rtb101', 'cpi-006  + soc', 'scta01', 'camostat mesylate', 'hyperbaric oxygen', 'brequinar', 'resveratrol', 'fp-025', 'best available treatment', 'tocilizumab (tcz)', 'favipiravir (3 + 1)', 'imu-838', 'placebo comparator', 'ino-4800', 'bucillamine', 'therapeutic anticoagulation', 'exo 2', 'carrimycin', 'hydroxychloroquine and azithromycin', 'standard of care (soc)', 'regn10933+regn10987', 'soc + placebo', 'peginterferon lambda-1a', 'standard covid-19 care', 'fuzheng huayu', 'doxycyclines', 'dwj1248', 'hb-admscs', 'guduchi ghan vati', 'dornase alfa [pulmozyme]', 'gx-i7', 'distilled water', 'silmitasertib', 'honey', 'drug: na-831 -', 'convalescent plasma transfusion', 'placebo-', 'td-0903', 'hydroxychloroquine sulfates', 'hydroxychloroquine sulfate loading dose', 'hydroxychloroquine sulfate  [plaquenil]', 'chemotherapy', 'ebselen', 'chloroquine sulfate', 'early-dexamethasone', 'hydroxychloroquine - weekly dosing', 'standard of care (soc)', 'exo 1', 'hydroxychloroquine + azithromycin', 'plasma', 'edp1815', 'ec-18', 'human immunoglobulin', 'resp301, a nitric oxide generating solution', 'plitidepsin 2./day', 'poly-iclc (hiltonol®)', 'remdesivir placebo', '0.9%sodium chloride', 'pul-042', 'standard care', 'molnupiravir', 'iodine complex', 'neuromuscular blocking agents', 'matching placebo', 'olokizumab', 'ensovibep', 'isoquercetin', 'n-acetyl cysteine', 'nors (nitric oxide releasing solution)', 'acebilustat', 'abidol hydrochloride', 'm5049', 'arb', 'low molecular weight heparin', 'xc7  single', 'ivermectin and doxycycline', 'at-527', 'ivermectin pill', 'angiotensin-(1-7)', 'rhtpo', 'nusepin®', 'alteplase  [activase]', 'unfractionated heparin', 'nitrogen', 'methylprednisolone sodium succinate', 'part a: upamostat', 'meplazumab for', 'unfractionated heparin', 'atyr1923', 'angiotensin converting enzyme inhibitor', 'azithromycins', 'interleukin-7', 'standard therapy', 'angiotensin 1-7']\n"
     ]
    }
   ],
   "source": [
    "all_drugs = list(set(repurposedf['name'].unique().tolist()).union(set(repurposedf['alias'].unique().tolist())))\n",
    "druglist_lower = [x.lower() for x in druglist]\n",
    "all_drugs_lower = [x.lower() for x in all_drugs]\n",
    "in_common = list(set(druglist_lower).intersection(set(all_drugs_lower)))\n",
    "missing = [x for x in druglist_lower if x not in in_common]\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580\n",
      "1060\n"
     ]
    }
   ],
   "source": [
    "DATAPATH = 'data/topicCategories/'\n",
    "\n",
    "repurpose_cts = drugs['_id'].loc[drugs['interventionName'].astype(str).str.lower().isin(in_common)].unique().tolist()\n",
    "drug_repurposing = clin_meta.loc[(clin_meta['interventionCategory']=='dietary supplement')|\n",
    "                                 (clin_meta['_id'].isin(repurpose_cts))]\n",
    "pharma_cts = drugs['_id'].loc[~drugs['_id'].isin(repurpose_cts)].unique().tolist()\n",
    "pharma_info = clin_meta.loc[clin_meta['_id'].isin(pharma_cts)]\n",
    "print(len(repurpose_cts))\n",
    "print(len(pharma_cts))\n",
    "with open(os.path.join(DATAPATH,'Repurposing.pickle'),'wb') as dumpfile:\n",
    "    pickle.dump(drug_repurposing,dumpfile)\n",
    "with open(os.path.join(DATAPATH,'Pharmaceutical Treatments.pickle'),'wb') as dumpfile:\n",
    "    pickle.dump(pharma_info,dumpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the mappings to generate datasets, ignoring drug and other broad categories \n",
    "which may require further downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eachintervention in intervention_map.keys():\n",
    "    tmpdf = clin_meta.loc[clin_meta['interventionCategory'].astype(str).str.contains(eachintervention)]\n",
    "    with open(os.path.join(DATAPATH,intervention_map[eachintervention]+'.pickle'),'wb') as outpath:\n",
    "        pickle.dump(tmpdf,outpath)\n",
    "\n",
    "## Do the same for design purpose\n",
    "for eachpurpose in designpurposemap.keys():\n",
    "    tmpdf = clin_meta.loc[clin_meta['designPrimaryPurpose'].astype(str).str.contains(eachpurpose)]\n",
    "    try:\n",
    "        originaldf = pickle.load(open(os.path.join(DATAPATH,designpurposemap[eachpurpose]+'.pickle'),'rb'))\n",
    "        combidf = pd.concat((originaldf,tmpdf),ignore_index=True)\n",
    "        combi.drop_duplicates(keep=\"first\",inplace=True)\n",
    "    except:\n",
    "        combidf = tmpdf\n",
    "    with open(os.path.join(DATAPATH,designpurposemap[eachpurpose]+'.pickle'),'wb') as outpath:\n",
    "        pickle.dump(combidf,outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply diagnostic keywords to subclassify diagnosis\n",
    "1. Use the designpurposemap and the intervention_map to identify CT's that should be classified as 'Diagnosis'\n",
    "2. Use the diagnostickeywords to search through the intervention names, or descriptions to further categorize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            _id                                           abstract  \\\n",
      "11  NCT04648709  Current data in the literature demonstrate tha...   \n",
      "12  NCT04648709  Current data in the literature demonstrate tha...   \n",
      "\n",
      "                                                 name description  \\\n",
      "11  Evaluation and Longitudinal Follow-up of Bioma...               \n",
      "12  Evaluation and Longitudinal Follow-up of Bioma...               \n",
      "\n",
      "   designPrimaryPurpose      studyType interventionCategory interventionName  \\\n",
      "11                       observational      diagnostic test          ELISPOT   \n",
      "12                       observational      diagnostic test      QUANTIFERON   \n",
      "\n",
      "                                                 text  \n",
      "11  evaluation and longitudinal follow up of bioma...  \n",
      "12  evaluation and longitudinal follow up of bioma...  \n"
     ]
    }
   ],
   "source": [
    "diagnosis = clin_meta.loc[((clin_meta['interventionCategory'].astype(str).str.contains('diagnostic test'))|\n",
    "                          (clin_meta['designPrimaryPurpose'].astype(str).str.contains('diagnostic'))|\n",
    "                          (clin_meta['designPrimaryPurpose'].astype(str).str.contains('screening')))].copy()\n",
    "print(diagnosis.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pathology/Radiology :  173\n",
      "Rapid Diagnostics :  197\n",
      "Virus Detection :  252\n",
      "Antibody Detection :  239\n",
      "Symptoms :  369\n"
     ]
    }
   ],
   "source": [
    "for eachdiag in diagnostickeywords.keys():\n",
    "    keywordlist = diagnostickeywords[eachdiag]\n",
    "    topicCategory = eachdiag\n",
    "    searchregex = re.compile('|'.join(keywordlist), re.IGNORECASE)\n",
    "    tmpdf = diagnosis.loc[((diagnosis['interventionName'].str.contains(searchregex))|\n",
    "                          (diagnosis['text'].str.contains(searchregex)))]\n",
    "    print(eachdiag,': ',len(tmpdf))\n",
    "    with open(os.path.join(DATAPATH,eachdiag.replace('/','_')+'.pickle'),'wb') as outpath:\n",
    "        pickle.dump(tmpdf,outpath)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply treatment keywords to subclassify Treatments\n",
    "Note that drugs are classified separately in Clinical Trials and so will be handled separately. Additionally, biologics are also separated out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltreatment = clin_meta.loc[(clin_meta['designPrimaryPurpose'].astype(str).str.contains('treatment'))]\n",
    "treatment = alltreatment.loc[((alltreatment['interventionCategory']!='drug')&\n",
    "                              (alltreatment['interventionCategory']!='biological')&\n",
    "                              (alltreatment['interventionCategory']!='genetic'))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vaccines :  66\n",
      "Medical Care :  693\n"
     ]
    }
   ],
   "source": [
    "for eachtreat in treatmentkeywords.keys():\n",
    "    keywordlist = treatmentkeywords[eachtreat]\n",
    "    topicCategory = eachtreat\n",
    "    searchregex = re.compile('|'.join(keywordlist), re.IGNORECASE)\n",
    "    tmpdf = treatment.loc[((treatment['interventionName'].str.contains(searchregex))|\n",
    "                          (treatment['text'].str.contains(searchregex)))]\n",
    "    print(eachtreat,': ',len(tmpdf))\n",
    "    try:\n",
    "        originaldf = pickle.load(open(os.path.join(DATAPATH,eachtreat+'.pickle'),'rb'))\n",
    "        combidf = pd.concat((originaldf,tmpdf),ignore_index=True)\n",
    "        combi.drop_duplicates(keep=\"first\",inplace=True)\n",
    "    except:\n",
    "        combidf = tmpdf\n",
    "\n",
    "    with open(os.path.join(DATAPATH,eachtreat+'.pickle'),'wb') as outpath:\n",
    "        pickle.dump(combidf,outpath)\n",
    "    combidf.to_csv(os.path.join(DATAPATH,eachtreat+'.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclassify Preventions\n",
    "1. Get frequencies of Intervention names listed under prevention\n",
    "2. Map frequent terms to prevention subcategories\n",
    "Note that vaccines are considered preventative, so include search for vaccines. Screening/early diagnosis can also be considered a preventative strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            _id                                           abstract  \\\n",
      "19  NCT04760743  This study is to assess the safety, reactogeni...   \n",
      "20  NCT04760743  This study is to assess the safety, reactogeni...   \n",
      "\n",
      "                                                 name  \\\n",
      "19  A Phase I, Placebo-controlled, Randomized, Obs...   \n",
      "20  A Phase I, Placebo-controlled, Randomized, Obs...   \n",
      "\n",
      "                                          description designPrimaryPurpose  \\\n",
      "19  This is a first-in-human, Phase I, randomized,...           prevention   \n",
      "20  This is a first-in-human, Phase I, randomized,...           prevention   \n",
      "\n",
      "         studyType interventionCategory  \\\n",
      "19  interventional           biological   \n",
      "20  interventional           biological   \n",
      "\n",
      "                                interventionName  \\\n",
      "19  NBP2001 adjuvanted with alum (RBD 30μg/dose)   \n",
      "20  NBP2001 adjuvanted with alum (RBD 50μg/dose)   \n",
      "\n",
      "                                                 text  \n",
      "19  a phase placebo controlled  randomized  observ...  \n",
      "20  a phase placebo controlled  randomized  observ...  \n",
      "1318\n"
     ]
    }
   ],
   "source": [
    "prevention = clin_meta.loc[clin_meta['designPrimaryPurpose'].astype(str).str.contains('prevention')].copy()\n",
    "print(prevention.head(n=2))\n",
    "print(len(prevention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WARD CSS', 'High volume evacuation (HVE)', 'Extraoral vacuum aspirator (EVA)', 'External evacuation device (EED)', 'COVID-19 barrier box', 'nasal spray', 'Extubation Advisor', 'CELLECTRA® 2000', 'UVC Irradiation', 'Fit test', 'Tensile strength', 'Cliniporator', 'N95 mask', 'Nordell single E-100 layer mask', 'Nordell double E-100 layer mask', 'non-contact magnetically-controlled capsule endoscopy', 'Medical Mask', 'N95 respirator', 'mouthrinse with bêta-cyclodextrin and citrox', 'mouthrinse without bêta-cyclodextrin and citrox', 'Biocontainment Device For Aerosol Generating Procedures (Biobox)', 'Control for aerosol generating procedures', 'Personal protective equipment', 'Cliniporator® and EPSGun', 'FFP2', 'Facial mask', 'MFS', 'V-CAMS (aka Jaspr)', 'tight-fitting of KF94 mask with clip', 'PEP flute', 'Threshold IMT device', 'Filtration Test', 'VESTA respirator', 'Conventional N95 respirator', 'Face mask', 'Inspiratory training device', 'Expiratory training device', 'Face Mask + Soap', 'Respiratory filter in-line placed with the standard mouthpiece', 'current IPAC-UHN PPE', 'modified IPAC-UHN PPE', 'Carrageenan nasal and throat spray', 'Saline nasal and throat spray']\n"
     ]
    }
   ],
   "source": [
    "print(prevention['interventionName'].loc[prevention['interventionCategory']=='device'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public Health Interventions :  78\n",
      "Individual Prevention :  315\n",
      "Vaccines :  613\n"
     ]
    }
   ],
   "source": [
    "for eachprevent in preventionkeywords.keys():\n",
    "    keywordlist = preventionkeywords[eachprevent]\n",
    "    topicCategory = eachprevent\n",
    "    searchregex = re.compile('|'.join(keywordlist), re.IGNORECASE)\n",
    "    tmpdf = prevention.loc[((prevention['interventionName'].str.contains(searchregex))|\n",
    "                            (prevention['text'].str.contains(searchregex)))]\n",
    "    print(eachprevent,': ',len(tmpdf))\n",
    "    try:\n",
    "        originaldf = pickle.load(open(os.path.join(DATAPATH,eachprevent+'.pickle'),'rb'))\n",
    "        combidf = pd.concat((originaldf,tmpdf),ignore_index=True)\n",
    "        combidf.drop_duplicates(keep=\"first\",inplace=True)\n",
    "    except:\n",
    "        combidf = tmpdf\n",
    "\n",
    "    with open(os.path.join(DATAPATH,eachprevent+'.pickle'),'wb') as outpath:\n",
    "        pickle.dump(combidf,outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take into account combi maps\n",
    "individual_prevention = prevention.loc[prevention['interventionCategory'].astype(str).str.contains('device')]\n",
    "originaldf = pickle.load(open(os.path.join(DATAPATH,'Individual Prevention.pickle'),'rb'))\n",
    "combidf = pd.concat((originaldf,individual_prevention),ignore_index=True)\n",
    "combidf.drop_duplicates(keep=\"first\",inplace=True)\n",
    "with open(os.path.join(DATAPATH,'Individual Prevention.pickle'),'wb') as outpath:\n",
    "    pickle.dump(combidf,outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Basic process\n",
    "## Create binary training set using Behavioral as an example\n",
    "original_data = read_csv('data/NCT_classification.csv', delimiter=',',header=0)\n",
    "\n",
    "with open('data/behavioral.txt','rb') as dmpfile:\n",
    "    behavioral = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/specificCats.txt','rb') as dompfile:\n",
    "    traindf = pickle.load(dompfile)\n",
    "\n",
    "with open('data/diagnosis.txt','rb') as dmpfile:\n",
    "    diagnosis = pickle.load(dmpfile)  \n",
    "\n",
    "with open('data/prevention.txt','rb') as dmpfile:\n",
    "    prevention = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/treatment.txt','rb') as dmpfile:\n",
    "    treatment = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/drug.txt','rb') as dmpfile:\n",
    "    drug = pickle.load(dmpfile)\n",
    "    \n",
    "pos_id = list(set(behavioral).union(set(traindf['_id'].loc[traindf['topicCategory']=='Behavioral Research'].tolist())))\n",
    "maybe_neg = list(set(traindf['_id'].loc[traindf['topicCategory']!='Behavioral Research'].tolist()).union(\n",
    "                 set(diagnosis).union(set(prevention).union(set(treatment)))))\n",
    "\n",
    "neg_id = [item  for item in maybe_neg if item not in pos_id]\n",
    "original_data = merge_texts(original_data)\n",
    "training_set_pos = original_data[['_id','text']].loc[(original_data['_id'].isin(pos_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "    \n",
    "training_set_pos['target']='behavioral'\n",
    "training_set_neg = original_data[['_id','text']].loc[(original_data['_id'].isin(neg_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "training_set_neg['target']='not behavioral'\n",
    "\n",
    "training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create binary training set using Diagnosis as an example\n",
    "original_data = read_csv('data/NCT_classification.csv', delimiter=',',header=0)\n",
    "\n",
    "with open('data/behavioral.txt','rb') as dmpfile:\n",
    "    behavioral = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/specificCats.txt','rb') as dompfile:\n",
    "    traindf = pickle.load(dompfile)\n",
    "\n",
    "with open('data/diagnosis.txt','rb') as dmpfile:\n",
    "    diagnosis = pickle.load(dmpfile)  \n",
    "\n",
    "with open('data/prevention.txt','rb') as dmpfile:\n",
    "    prevention = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/treatment.txt','rb') as dmpfile:\n",
    "    treatment = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/drug.txt','rb') as dmpfile:\n",
    "    drug = pickle.load(dmpfile)\n",
    "\n",
    "pos_id = list(set(diagnosis).union(set(traindf['_id'].loc[traindf['topicCategory']=='Diagnosis'].tolist())))\n",
    "maybe_neg = list(set(traindf['_id'].loc[traindf['topicCategory']!='Diagnosis'].tolist()).union(\n",
    "                 set(treatment).union(set(prevention))))\n",
    "\n",
    "neg_id = [item  for item in maybe_neg if item not in pos_id]\n",
    "original_data = merge_texts(original_data)\n",
    "training_set_pos = original_data[['_id','text']].loc[(original_data['_id'].isin(pos_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "    \n",
    "training_set_pos['target']='diagnosis'\n",
    "training_set_neg = original_data[['_id','text']].loc[(original_data['_id'].isin(neg_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "training_set_neg['target']='not diagnosis'\n",
    "\n",
    "training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create binary training set using Prevention as an example\n",
    "original_data = read_csv('data/NCT_classification.csv', delimiter=',',header=0)\n",
    "\n",
    "with open('data/behavioral.txt','rb') as dmpfile:\n",
    "    behavioral = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/specificCats.txt','rb') as dompfile:\n",
    "    traindf = pickle.load(dompfile)\n",
    "\n",
    "with open('data/diagnosis.txt','rb') as dmpfile:\n",
    "    diagnosis = pickle.load(dmpfile)  \n",
    "\n",
    "with open('data/prevention.txt','rb') as dmpfile:\n",
    "    prevention = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/treatment.txt','rb') as dmpfile:\n",
    "    treatment = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/drug.txt','rb') as dmpfile:\n",
    "    drug = pickle.load(dmpfile)\n",
    "    \n",
    "pos_id = list(set(prevention).union(set(traindf['_id'].loc[(traindf['topicCategory']=='Prevention')|\n",
    "                                                           (traindf['topicCategory']=='Individual Prevention')].tolist())))\n",
    "maybe_neg = list(set(traindf['_id'].loc[((traindf['topicCategory']!='Prevention')&\n",
    "                                        (traindf['topicCategory']!='Individual Prevention'))].tolist()).union(\n",
    "                 set(treatment).union(set(diagnosis))))\n",
    "\n",
    "neg_id = [item  for item in maybe_neg if item not in pos_id]\n",
    "original_data = merge_texts(original_data)\n",
    "training_set_pos = original_data[['_id','text']].loc[(original_data['_id'].isin(pos_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "    \n",
    "training_set_pos['target']='prevention'\n",
    "training_set_neg = original_data[['_id','text']].loc[(original_data['_id'].isin(neg_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "training_set_neg['target']='not prevention'\n",
    "\n",
    "training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create binary training set using Treatment as an example\n",
    "original_data = read_csv('data/NCT_classification.csv', delimiter=',',header=0)\n",
    "\n",
    "with open('data/behavioral.txt','rb') as dmpfile:\n",
    "    behavioral = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/specificCats.txt','rb') as dompfile:\n",
    "    traindf = pickle.load(dompfile)\n",
    "\n",
    "with open('data/diagnosis.txt','rb') as dmpfile:\n",
    "    diagnosis = pickle.load(dmpfile)  \n",
    "\n",
    "with open('data/prevention.txt','rb') as dmpfile:\n",
    "    prevention = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/treatment.txt','rb') as dmpfile:\n",
    "    treatment = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/drug.txt','rb') as dmpfile:\n",
    "    drug = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/basic science.txt','rb') as dmpfile:\n",
    "    basic_science = pickle.load(dmpfile)\n",
    "\n",
    "with open('data/observational.txt','rb') as dmpfile:\n",
    "    observation = pickle.load(dmpfile)\n",
    "\n",
    "    \n",
    "pos_id = list(set(treatment).union(set(traindf['_id'].loc[((traindf['topicCategory']=='Treatment')|\n",
    "                                                           (traindf['topicCategory']=='Vaccines')|\n",
    "                                                           (traindf['topicCategory']=='Biologics')|\n",
    "                                                           (traindf['topicCategory']=='Medical Care'))].tolist()))-set(basic_science)-set(observation))\n",
    "maybe_neg = list(set(traindf['_id'].loc[((traindf['topicCategory']!='Treatment')&\n",
    "                                         (traindf['topicCategory']!='Vaccines')&\n",
    "                                         (traindf['topicCategory']!='Biologics')&\n",
    "                                         (traindf['topicCategory']!='Medical Care'))].tolist()).union(\n",
    "                 set(prevention).union(set(diagnosis).union(set(basic_science).union(set(observation))))))\n",
    "\n",
    "neg_id = [item  for item in maybe_neg if item not in pos_id]\n",
    "original_data = merge_texts(original_data)\n",
    "training_set_pos = original_data[['_id','text']].loc[(original_data['_id'].isin(pos_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "    \n",
    "training_set_pos['target']='treatment'\n",
    "training_set_neg = original_data[['_id','text']].loc[(original_data['_id'].isin(neg_id))&\n",
    "                                                     (~original_data['text'].isna())]\n",
    "training_set_neg['target']='not treatment'\n",
    "\n",
    "training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/specificCats.txt','rb') as dompfile:\n",
    "    traindf = pickle.load(dompfile)\n",
    "\n",
    "print(len(traindf))\n",
    "print(traindf.groupby('topicCategory').size())\n",
    "print(len(traindf['_id'].loc[((traindf['topicCategory']!='Treatment')&\n",
    "                                         (traindf['topicCategory']!='Vaccines')&\n",
    "                                         (traindf['topicCategory']!='Biologics')&\n",
    "                                         (traindf['topicCategory']!='Medical Care'))]))\n",
    "\n",
    "print(len(traindf['_id'].loc[((traindf['topicCategory']=='Treatment')|\n",
    "                                                           (traindf['topicCategory']=='Vaccines')|\n",
    "                                                           (traindf['topicCategory']=='Biologics')|\n",
    "                                                           (traindf['topicCategory']=='Medical Care'))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_set_pos))\n",
    "print(len(training_set_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Vectorize the text for classifier\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(training_set['text'])\n",
    "features = vectorizer.get_feature_names()\n",
    "print(X.shape)\n",
    "\n",
    "#### Split the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, training_set.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Classify training text as in category or not in category\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "report = classification_report(y_test,y_pred,output_dict=True)\n",
    "print(pd.DataFrame(report))\n",
    "\n",
    "probs = classifier.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "print(auc)\n",
    "print('[[true neg     false pos]]')\n",
    "print('[[false neg     true pos]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand beyond training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run the classifier on the entire training dataset without splitting\n",
    "####Vectorize the training set for classifier\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(training_set['text'])\n",
    "features = vectorizer.get_feature_names()\n",
    "print(X.shape)\n",
    "\n",
    "## Save the vectorizer\n",
    "#vectorizerfile = \"results/models/behavioral_vectorizer.pickle\"\n",
    "#vectorizerfile = \"results/models/diagnosis_vectorizer.pickle\"\n",
    "#vectorizerfile = \"results/models/prevention_vectorizer.pickle\"\n",
    "vectorizerfile = \"results/models/treatment_vectorizer.pickle\"\n",
    "pickle.dump(vectorizer, open(vectorizerfile, \"wb\"))\n",
    "\n",
    "#### train the model on all the data\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=None)\n",
    "classifier.fit(X, training_set.target)\n",
    "\n",
    "## Save the Model\n",
    "#filename = 'results/models/behavioral_randomforest.sav'\n",
    "#filename = 'results/models/diagnosis_randomforest.sav'\n",
    "#filename = 'results/models/prevention_randomforest.sav'\n",
    "filename = 'results/models/treatment_randomforest.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved models\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Apply the model\n",
    "## Pull out all the data \n",
    "idlist = get_source_ids()\n",
    "textdf = batch_fetch_meta(idlist)\n",
    "textdf = merge_texts(textdf)\n",
    "\n",
    "## Remove records with no text\n",
    "nonan = textdf.loc[((~textdf['text'].isna())&\n",
    "                   (textdf['text'].str.len()>5))]\n",
    "\n",
    "print(len(idlist), len(nonan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Vectorize the text based on the previously trained vectorizer and run the classifier\n",
    "labels = nonan['_id']\n",
    "M = vectorizer.transform(nonan['text'])\n",
    "prediction = classifier.predict(M)\n",
    "\n",
    "## Save the results\n",
    "#classifier_results = pd.DataFrame(list(zip(labels, prediction)), columns =['_id', 'behavioral_prediction'])\n",
    "#print(classifier_results.head(n=2))\n",
    "#classifier_results.to_csv('results/predictions/behavioral_randomforest.tsv',\n",
    "#                          sep='\\t', header=True)\n",
    "\n",
    "#classifier_results = pd.DataFrame(list(zip(labels, prediction)), columns =['_id', 'diagnosis_prediction'])\n",
    "#print(classifier_results.head(n=2))\n",
    "#classifier_results.to_csv('results/predictions/diagnosis_randomforest.tsv',\n",
    "#                          sep='\\t', header=True)\n",
    "\n",
    "#classifier_results = pd.DataFrame(list(zip(labels, prediction)), columns =['_id', 'prevention_prediction'])\n",
    "#print(classifier_results.head(n=2))\n",
    "#classifier_results.to_csv('results/predictions/prevention_randomforest.tsv',\n",
    "#                          sep='\\t', header=True)\n",
    "\n",
    "classifier_results = pd.DataFrame(list(zip(labels, prediction)), columns =['_id', 'treatment_prediction'])\n",
    "print(classifier_results.head(n=2))\n",
    "classifier_results.to_csv('results/predictions/treatment_randomforest.tsv',\n",
    "                          sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_results = classifier_results.groupby('treatment_prediction').size().reset_index(name='counts')\n",
    "print(inspect_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
